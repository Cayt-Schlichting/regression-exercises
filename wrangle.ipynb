{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e22c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from env import get_db_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251afd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write some SQL - expect 2152863 rows\n",
    "sql = \"\"\"\n",
    "SELECT bedroomcnt as bed,\n",
    "    bathroomcnt as bath, \n",
    "    calculatedfinishedsquarefeet as sf, \n",
    "    taxvaluedollarcnt as value, \n",
    "    yearbuilt, \n",
    "    taxamount, \n",
    "    fips\n",
    "FROM properties_2017\n",
    "    JOIN propertylandusetype USING(propertylandusetypeid)\n",
    "WHERE propertylandusedesc = 'Single Family Residential';\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_backup = pd.read_sql(sql,get_db_url('zillow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77040741",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #rows match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da0b65",
   "metadata": {},
   "source": [
    "### Investigating the nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5e50e",
   "metadata": {},
   "source": [
    "First, let's look at the nulls of our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd222007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.value.isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0452f6",
   "metadata": {},
   "source": [
    "Only 500 of 2 million, plus we don't want to be imputing our target. \n",
    "\n",
    "**ACTION:** Drop all rows where target variable is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e792b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_null_ind =  df[df.value.isna()].index\n",
    "df.drop(index=value_null_ind,inplace=True)\n",
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0192ad",
   "metadata": {},
   "source": [
    "Based on the above table, we have fips for every row which makes sense as every parcelid (pk from orginial table) should have a fips.  \n",
    "\n",
    "The next highest non-null columns are bed and bath.  so let's investigate those first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278fa878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.bed.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ed9e7",
   "metadata": {},
   "source": [
    "Looking at the above table, this subset of rows have minimal information.  If there is no bed or bath AND half of the remaining columns don't exist, we should just drop.\n",
    "\n",
    "**ACTION:** Drop all rows where df.bed is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f3a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_null_ind = df[df.bed.isna()].index\n",
    "df.drop(index=bed_null_ind,inplace=True)\n",
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8e800",
   "metadata": {},
   "source": [
    "Now let's see how many are in the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sf.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.yearbuilt.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.taxamount.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check out some overlap\n",
    "hist = df.count(axis=1,numeric_only=False)\n",
    "plt.hist(hist[hist<7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a7e3c",
   "metadata": {},
   "source": [
    "Looking at the above, a decent number of these are only missing one piece of data.  However, since we are still only looking at ~12k total rows, I am going to go ahead and drop them all.  Ideally, I'd check some of this with stakeholders.  My primary concern is that we may be inadvertently trimming a particular geographic area (perhaps one with poor tax amount reporting to the county)\n",
    "\n",
    "**ACTION:** Drop all nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ba61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7abfd",
   "metadata": {},
   "source": [
    "### Look at the distributions of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d3a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See on logarythmic scale to better see outliers\n",
    "plt.figure(figsize=(10,15))\n",
    "ct=0\n",
    "for c in df.columns:\n",
    "    ct +=1\n",
    "    plt.subplot(7,2,ct)\n",
    "    plt.hist(df[c])\n",
    "    plt.title(c+'_log')\n",
    "    plt.yscale('log')\n",
    "    ct +=1\n",
    "    plt.subplot(7,2,ct)\n",
    "    plt.hist(df[c])\n",
    "    plt.title(c)\n",
    "    plt.ylim((0,10))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0423971f",
   "metadata": {},
   "source": [
    "Definitely some bed, bath, tax, value and year outliers.  I don't want to snap any values in.  I also think it may be best to ignore bed/bath outliers first, then see if addressing sf or value will handle those.\n",
    "\n",
    "I want to avoid trimming by my target variable, so first I'll trim by sf and hope that it also helps address outliers on the value:\n",
    "- Since Zillow estimates are not expected to be utilized by the top 1%, I want to cut some of the expensive houses. We'll cut the top .1% of sf homes off the dataset.  NOTE: I recognized that sf is being used as a proxy for value, but it's one method of avoiding trimming by our target so that we can better specify how the model was trained in a useful manner\n",
    "- Since the data is skewed right, I want to trim less off the left.  After considering a few different cutoffs, I found that california code restricts minimum dwelling size to 120 sq ft. \n",
    "  - \"Every dwelling unit shall have at least one room that shall have not less than 120 square feet (13.9 m2) of net floor area\"\n",
    "  \n",
    "**ACTION:** Drop rows with the top .1% of sf or an sf of less than 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop top 1% of sf\n",
    "df = df[df.sf<df.sf.quantile(.999)]\n",
    "\n",
    "#drop anything less than 120 sf\n",
    "df = df[df.sf>=120]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11acf567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69930117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See on logarythmic scale to better see outliers\n",
    "plt.figure(figsize=(10,15))\n",
    "ct=0\n",
    "for c in df.columns:\n",
    "    ct +=1\n",
    "    plt.subplot(7,2,ct)\n",
    "    plt.hist(df[c])\n",
    "    plt.title(c+'_log')\n",
    "    plt.yscale('log')\n",
    "    ct +=1\n",
    "    plt.subplot(7,2,ct)\n",
    "    plt.hist(df[c])\n",
    "    plt.title(c)\n",
    "    plt.ylim((0,10))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e4514",
   "metadata": {},
   "source": [
    "Even after using sf trimming, we still have quite a few outliers in each category.  Because of that, I'll do more trimming on the high side.  Since Zillow's target customers aren't the super rich, I feel comfortable trimming 10+ bedrooms, 10+ bathrooms and 2+ million value.  While we wanted to avoid any triming by value, it is import to get rid of these extreme outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee27b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.bed > 9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.bath>9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91604ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.value>2_000_000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e38241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.value>2_000_000) | (df.bath >9) | (df.bed >10)].shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260964b",
   "metadata": {},
   "source": [
    "Even with the extra trimming, that only accounts for <2% of the data.  \n",
    "\n",
    "**Action:** Drop all rows with 10+ beds, 10+ baths, or a value of 10+ million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.value < 2_000_000) & (df.bath < 10) & (df.bed <10)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b5700",
   "metadata": {},
   "source": [
    "### Now encode our categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4408de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fips.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only categorical is FIPS.  I want to map then encode so that I have common sense \n",
    "# names for EDA and easy to read columns for the model\n",
    "\n",
    "#map to county names\n",
    "df['county'] = df.fips.map({6037: 'LosAngeles_CA',6059:'Orange_CA',6111:'Ventura_CA'})\n",
    "#encode into dummy df\n",
    "d_df = pd.get_dummies(df['county'],drop_first=True)\n",
    "#concat dummy df to the rest\n",
    "df = pd.concat([df,d_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c610f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50670300",
   "metadata": {},
   "source": [
    "### See if other datatypes are appropriate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab488b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bed.value_counts() #can be integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.yearbuilt % 1).value_counts() # can be integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4afd7",
   "metadata": {},
   "source": [
    "Convert those two to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bed = df.bed.astype(int)\n",
    "df.yearbuilt = df.yearbuilt.astype(int)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d509be8",
   "metadata": {},
   "source": [
    "### Drop any unecessary columns\n",
    "\n",
    "taxamount is proportional to tax value.  In addition, tax amount follows tax value.  Major changes in tax amount often come after the sale of a home.  So for Zillow's purposes, we don't want this as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of original fips column\n",
    "df.drop(columns=['fips','taxamount'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e056087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e79d7",
   "metadata": {},
   "source": [
    "### Now reorder columns for easier EDA/model splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf696294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex(columns=['value', 'county', 'bed', 'bath', 'sf', 'yearbuilt', 'Orange_CA', 'Ventura_CA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa2700",
   "metadata": {},
   "source": [
    "### Dropped work into function in wrangle.py\n",
    "\n",
    "### Now test the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = wrangle.getZillowData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b539fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te, val = wrangle.prep_zillow(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c168695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43c68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249c408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
